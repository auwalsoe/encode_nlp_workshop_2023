{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief introduction to text corpus pre-processing for machine learning applications.\n",
    "### By André Walsøe, Data scientist/Head Engineer Oslo University Library 2018\n",
    "This is a brief introduction to text corpus pre-processing for machine learning applications produced for the Research Bazaar, \"Hands-on Workshop: Exploring Research Data with Artificial Intelligence and Design Thinking\" at UiO, January 11th 2019. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals:\n",
    "1. Understand basic techniques for removing noise from text data and how to apply these techniques on a corpus.\n",
    "2. Understand techniques for improving data representation for machine learning applications, using lemmatization, stemming, stopword removal etc.\n",
    "3. Understand the two main forms of feature extraction for text data: count vectorization and Tf-idf.\n",
    "4. Be able to apply the above learned techniques on simple problems using python. \n",
    "![alt text](https://www.kdnuggets.com/wp-content/uploads/text-data-task-framework.png \"Logo Title Text 1\")\n",
    "source: https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Heraklous to2 Pompeius greeting and all good wishes for his health. As soon as the letter reaches you,\\n\n",
    "come immediately. Since Ap92eis died, Se@rapous never stops pestering me at home. Since she demands: \\n\n",
    "'Produce th#e+ memoranda', she stands the-re saying: 'Two memoranda'. I do not give them. Argyrios has utterly\\n \n",
    "confir=med us, saying: 'You h23a21ve not paid the tax on sales'. If the letter reaches you, c54ome immediately. If not,\\n\n",
    "I shall leave the house. If you don't come, I shall arrange about the house (?) and come to y9ou in order that...\\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Lower text\n",
    "Lowering all the capital-letters in the text input. This is done in order to make the computer understand that the \n",
    "words \"Artificial\" means the same as \"artificial\" etc. This will decrease the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"heraklous to2 pompeius greeting and all good wishes for his health. as soon as the letter reaches you,\\n\\ncome immediately. since ap92eis died, se@rapous never stops pestering me at home. since she demands: \\n\\n'produce th#e+ memoranda', she stands the-re saying: 'two memoranda'. i do not give them. argyrios has utterly\\n \\nconfir=med us, saying: 'you h23a21ve not paid the tax on sales'. if the letter reaches you, c54ome immediately. if not,\\n\\ni shall leave the house. if you don't come, i shall arrange about the house (?) and come to y9ou in order that...\\n\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Remove linebreak\n",
    "Some texts contains the linebreak symbol \"\\n\". This symbol is only there to show that it is the end of the line and the start of a new line. This contains no semantical information is therefore removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"heraklous to2 pompeius greeting and all good wishes for his health. as soon as the letter reaches you,  come immediately. since ap92eis died, se@rapous never stops pestering me at home. since she demands:   'produce th#e+ memoranda', she stands the-re saying: 'two memoranda'. i do not give them. argyrios has utterly   confir=med us, saying: 'you h23a21ve not paid the tax on sales'. if the letter reaches you, c54ome immediately. if not,  i shall leave the house. if you don't come, i shall arrange about the house (?) and come to y9ou in order that... \""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.replace(\"\\n\", \" \")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Remove all numbers\n",
    "In this step we remove all the numbers. This is a choice that needs to be considered from application to application. In some applications numbers may contain significant information, and in others not. In this examples the numbers are removed as we consider them as not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"heraklous to pompeius greeting and all good wishes for his health. as soon as the letter reaches you,  come immediately. since apeis died, se@rapous never stops pestering me at home. since she demands:   'produce th#e+ memoranda', she stands the-re saying: 'two memoranda'. i do not give them. argyrios has utterly   confir=med us, saying: 'you have not paid the tax on sales'. if the letter reaches you, come immediately. if not,  i shall leave the house. if you don't come, i shall arrange about the house (?) and come to you in order that... \""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_numbers(text):\n",
    "    split_text = list(text)\n",
    "    split_text\n",
    "    split_text = [x for x in split_text if not x.isdigit()]\n",
    "    text = ''.join(split_text)\n",
    "    return text\n",
    "text = remove_numbers(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Remove all non-alphabetical symbols\n",
    "In this step all non-alphabetical symbols are removed using regular expressions (often shortened to regex). Regex is a string pattern search algorithm. This is also application dependent, but in most applications this is removed (or at least a selection of symbols) as it adds noise to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heraklous to pompeius greeting and all good wishes for his health as soon as the letter reaches you  come immediately since apeis died serapous never stops pestering me at home since she demands   produce the memoranda she stands there saying two memoranda i do not give them argyrios has utterly   confirmed us saying you have not paid the tax on sales if the letter reaches you come immediately if not  i shall leave the house if you dont come i shall arrange about the house  and come to you in order that '"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def remove_non_alpha(text):\n",
    "    regex = re.compile('[^a-zA-Z\\s]')\n",
    "    text = regex.sub('', text)\n",
    "    return text\n",
    "text = remove_non_alpha(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some functions for statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## Calculate average word length\n",
    "def avg_word_len(tokenized_array):\n",
    "    len_arr = [len(x) for x in tokenized_array]\n",
    "    avg = np.average(len_arr)\n",
    "    return avg\n",
    "def vocab_size(tokenized_array):\n",
    "    return len(set(tokenized_array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Tokenize text\n",
    "Tokenization is the process if splitting a string into words, sentences or other units dependent on the applications.\n",
    "Example using word tokenization:\n",
    "\n",
    "1) Input string: \"This is an example\"\n",
    "\n",
    "2) Output array: [\"this\", \"is\", \"an\", \"example\"]\n",
    "\n",
    "For this task we will use a platform called NLTK, Natural Language Toolkit.\n",
    "\n",
    "Description from their website:\n",
    "\n",
    "> NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use \n",
    ">  interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing \n",
    ">  libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for \n",
    ">  industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "For further information on how to use NLTK and also how to do Natural Language Processing in general, check this out:\n",
    "http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word_length: 4.456521739130435\n",
      "Length of text: 92\n",
      "Vocabulary size: 62\n",
      "['heraklous', 'to', 'pompeius', 'greeting', 'and', 'all', 'good', 'wishes', 'for', 'his', 'health', 'as', 'soon', 'as', 'the', 'letter', 'reaches', 'you', 'come', 'immediately', 'since', 'apeis', 'died', 'serapous', 'never', 'stops', 'pestering', 'me', 'at', 'home', 'since', 'she', 'demands', 'produce', 'the', 'memoranda', 'she', 'stands', 'there', 'saying', 'two', 'memoranda', 'i', 'do', 'not', 'give', 'them', 'argyrios', 'has', 'utterly', 'confirmed', 'us', 'saying', 'you', 'have', 'not', 'paid', 'the', 'tax', 'on', 'sales', 'if', 'the', 'letter', 'reaches', 'you', 'come', 'immediately', 'if', 'not', 'i', 'shall', 'leave', 'the', 'house', 'if', 'you', 'dont', 'come', 'i', 'shall', 'arrange', 'about', 'the', 'house', 'and', 'come', 'to', 'you', 'in', 'order', 'that']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def tokenize_text(text):\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    return tokenized_text\n",
    "tokenized_text = tokenize_text(text)\n",
    "print(\"Average word_length: {}\".format(avg_word_len(tokenized_text)))\n",
    "print(\"Length of text: {}\".format(len(tokenized_text)))\n",
    "print(\"Vocabulary size: {}\".format(vocab_size(tokenized_text)))\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Remove stopwords\n",
    "In this step we remove stopwords from the text. Stopwords is a selection of common words that add little to no information about the content of the text. The biggest advantage is that it removes high frequency terms that carry little semantic importance. Another advantage is that it reduces the dimensionality of the data, which results in reducing computation. The disadvantage can be that it removes some verbs that may have importance in some settings. For example in Shakespeare, the phrase \"to be or not to be\" would be removed. \n",
    "\n",
    "In the section below we can see a typical selection of stopwords for the english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('english')\n",
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word_length before stopword removal: 4.456521739130435\n",
      "Length of text before stopword removal: 92\n",
      "Vocabulary size before stopword removal: 62\n",
      "------------------------------------\n",
      "Average word_length after stopword removal: 5.9\n",
      "Length of text after stopword removal: 50\n",
      "Vocabulary size after stopword removal: 39\n",
      "['heraklous', 'pompeius', 'greeting', 'good', 'wishes', 'health', 'soon', 'letter', 'reaches', 'come', 'immediately', 'since', 'apeis', 'died', 'serapous', 'never', 'stops', 'pestering', 'home', 'since', 'demands', 'produce', 'memoranda', 'stands', 'saying', 'two', 'memoranda', 'give', 'argyrios', 'utterly', 'confirmed', 'us', 'saying', 'paid', 'tax', 'sales', 'letter', 'reaches', 'come', 'immediately', 'shall', 'leave', 'house', 'dont', 'come', 'shall', 'arrange', 'house', 'come', 'order']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_text):\n",
    "    tokenized_text = [x for x in tokenized_text if x not in stopWords]\n",
    "    return tokenized_text\n",
    "print(\"Average word_length before stopword removal: {}\".format(avg_word_len(tokenized_text)))\n",
    "print(\"Length of text before stopword removal: {}\".format(len(tokenized_text)))\n",
    "print(\"Vocabulary size before stopword removal: {}\".format(vocab_size(tokenized_text)))\n",
    "print(\"------------------------------------\")\n",
    "tokenized_text = remove_stopwords(tokenized_text)\n",
    "\n",
    "print(\"Average word_length after stopword removal: {}\".format(avg_word_len(tokenized_text)))\n",
    "print(\"Length of text after stopword removal: {}\".format(len(tokenized_text)))\n",
    "print(\"Vocabulary size after stopword removal: {}\".format(vocab_size(tokenized_text)))\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Lemmatization\n",
    "Simply explained lemmatization is the process of converting a word to it's dictionary form. \n",
    "Example from wikipedia https://en.wikipedia.org/wiki/Lemmatisation:\n",
    ">For instance:\n",
    ">1. The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    ">2. The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n",
    ">3. The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context; >e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation attempts to select the >correct lemma depending on the context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word_length before lemmatization: 5.9\n",
      "Length of text before lemmatization: 50\n",
      "Vocabulary size before lemmatization: 39\n",
      "['heraklous', 'pompeius', 'greeting', 'good', 'wish', 'health', 'soon', 'letter', 'reach', 'come', 'immediately', 'since', 'apeis', 'died', 'serapous', 'never', 'stop', 'pestering', 'home', 'since', 'demand', 'produce', 'memoranda', 'stand', 'saying', 'two', 'memoranda', 'give', 'argyrios', 'utterly', 'confirmed', 'u', 'saying', 'paid', 'tax', 'sale', 'letter', 'reach', 'come', 'immediately', 'shall', 'leave', 'house', 'dont', 'come', 'shall', 'arrange', 'house', 'come', 'order']\n",
      "Average word_length after lemmatization: 5.68\n",
      "Length of text after lemmatization: 50\n",
      "Vocabulary size after lemmatization: 39\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_text(tokenized_text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [wordnet_lemmatizer.lemmatize(x) for x in tokenized_text]\n",
    "    return lemmatized_text\n",
    "\n",
    "print(\"Average word_length before lemmatization: {}\".format(avg_word_len(tokenized_text)))\n",
    "print(\"Length of text before lemmatization: {}\".format(len(tokenized_text)))\n",
    "print(\"Vocabulary size before lemmatization: {}\".format(vocab_size(tokenized_text)))\n",
    "\n",
    "tokenized_text = lemmatize_text(tokenized_text)\n",
    "print(tokenized_text)\n",
    "\n",
    "print(\"Average word_length after lemmatization: {}\".format(avg_word_len(tokenized_text)))\n",
    "print(\"Length of text after lemmatization: {}\".format(len(tokenized_text)))\n",
    "print(\"Vocabulary size after lemmatization: {}\".format(vocab_size(tokenized_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Stemming\n",
    "Stemming is the process of collapsing the morphological variants of a word together. For example, the without stemming the terms process, processing and processed will be treated as distinct items with separate term frequences (Jurafsky). The biggest advantage with applying stemming is that it allows matching of all morphological variants of the term. The disadvantage is that it throws away useful differences. For example the words \"stocks\" and \"stockings\" will be reduced to the term \"stock\", which is two totally different words. Another example is the word \"university\" and \"universe\" which will be reduced to \"univers\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word_length before stemming: 5.68\n",
      "Length of text before stemming: 50\n",
      "Vocabulary size before stemming: 39\n",
      "Average word_length after stemming: 4.86\n",
      "Length of text after stemming: 50\n",
      "Vocabulary size after stemming: 39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['herakl',\n",
       " 'pompeiu',\n",
       " 'greet',\n",
       " 'good',\n",
       " 'wish',\n",
       " 'health',\n",
       " 'soon',\n",
       " 'letter',\n",
       " 'reach',\n",
       " 'come',\n",
       " 'immedi',\n",
       " 'sinc',\n",
       " 'apei',\n",
       " 'die',\n",
       " 'serap',\n",
       " 'never',\n",
       " 'stop',\n",
       " 'pester',\n",
       " 'home',\n",
       " 'sinc',\n",
       " 'demand',\n",
       " 'produc',\n",
       " 'memoranda',\n",
       " 'stand',\n",
       " 'say',\n",
       " 'two',\n",
       " 'memoranda',\n",
       " 'give',\n",
       " 'argyrio',\n",
       " 'utterli',\n",
       " 'confirm',\n",
       " 'u',\n",
       " 'say',\n",
       " 'paid',\n",
       " 'tax',\n",
       " 'sale',\n",
       " 'letter',\n",
       " 'reach',\n",
       " 'come',\n",
       " 'immedi',\n",
       " 'shall',\n",
       " 'leav',\n",
       " 'hous',\n",
       " 'dont',\n",
       " 'come',\n",
       " 'shall',\n",
       " 'arrang',\n",
       " 'hous',\n",
       " 'come',\n",
       " 'order']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def stem_text(tokenized_text):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    stemmed_text = [porter_stemmer.stem(x) for x in tokenized_text]\n",
    "    return stemmed_text\n",
    "\n",
    "print(\"Average word_length before stemming: {}\".format(avg_word_len(tokenized_text)))\n",
    "print(\"Length of text before stemming: {}\".format(len(tokenized_text)))\n",
    "print(\"Vocabulary size before stemming: {}\".format(vocab_size(tokenized_text)))\n",
    "\n",
    "tokenized_text = stem_text(tokenized_text)\n",
    "\n",
    "\n",
    "print(\"Average word_length after stemming: {}\".format(avg_word_len(tokenized_text)))\n",
    "print(\"Length of text after stemming: {}\".format(len(tokenized_text)))\n",
    "print(\"Vocabulary size after stemming: {}\".format(vocab_size(tokenized_text)))\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess Text Corpus\n",
    "A corpus is a structured set of texts which can be used for statistical analysis or similar. Below we have a small corpus of just 3 texts to experiment with. In this section I will show how we can apply the pre-processing methods shown above on a corpus. We will re-use the functions written above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is no friend \\nas2 loy1al as21 a book',\n",
       " '3There are \\nworse cri!m2es than burning books. One of them is not reading them.',\n",
       " 'I discovered me in the -231 !22 library. \\nI went to find me in the library.']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_corpus = [\"There is no friend \\nas2 loy1al as21 a book\", \"3There are \\nworse cri!m2es than burning books. One of them is not reading them.\", \"I discovered me in the -231 !22 library. \\nI went to find me in the library.\"]\n",
    "example_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Lower text\n",
    "How to lower all text in a a corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there is no friend \\nas2 loy1al as21 a book',\n",
       " '3there are \\nworse cri!m2es than burning books. one of them is not reading them.',\n",
       " 'i discovered me in the -231 !22 library. \\ni went to find me in the library.']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_corpus = [x.lower() for x in example_corpus]\n",
    "example_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Remove Linebreak\n",
    "Remove linebreak symbols from text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there is no friend  as2 loy1al as21 a book',\n",
       " '3there are  worse cri!m2es than burning books. one of them is not reading them.',\n",
       " 'i discovered me in the -231 !22 library.  i went to find me in the library.']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_corpus = [x.replace(\"\\n\", \" \") for x in example_corpus]\n",
    "example_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Remove All Numbers\n",
    "Removing all numbers from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there is no friend  as loyal as a book',\n",
       " 'there are  worse cri!mes than burning books. one of them is not reading them.',\n",
       " 'i discovered me in the - ! library.  i went to find me in the library.']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_corpus = [remove_numbers(x) for x in example_corpus]\n",
    "example_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Remove all non-alphabetical symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there is no friend  as loyal as a book',\n",
       " 'there are  worse crimes than burning books one of them is not reading them',\n",
       " 'i discovered me in the   library  i went to find me in the library']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_corpus = [remove_non_alpha(x) for x in example_corpus]\n",
    "example_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['there', 'is', 'no', 'friend', 'as', 'loyal', 'as', 'a', 'book'],\n",
       " ['there',\n",
       "  'are',\n",
       "  'worse',\n",
       "  'crimes',\n",
       "  'than',\n",
       "  'burning',\n",
       "  'books',\n",
       "  'one',\n",
       "  'of',\n",
       "  'them',\n",
       "  'is',\n",
       "  'not',\n",
       "  'reading',\n",
       "  'them'],\n",
       " ['i',\n",
       "  'discovered',\n",
       "  'me',\n",
       "  'in',\n",
       "  'the',\n",
       "  'library',\n",
       "  'i',\n",
       "  'went',\n",
       "  'to',\n",
       "  'find',\n",
       "  'me',\n",
       "  'in',\n",
       "  'the',\n",
       "  'library']]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_example_corpus = [tokenize_text(x) for x in example_corpus]\n",
    "tokenized_example_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some statistics functions\n",
    "import numpy as np\n",
    "## Calculate average word length\n",
    "def avg_word_len_corpus(tokenized_corpus):\n",
    "    merged_corpus = []\n",
    "    for l in tokenized_corpus:\n",
    "        merged_corpus += l\n",
    "    \n",
    "    len_arr = [len(x) for x in merged_corpus]\n",
    "    \n",
    "    avg = np.average(len_arr)\n",
    "    return avg\n",
    "def get_text_lengths(tokenized_corpus):\n",
    "    len_arr = [len(x) for x in tokenized_corpus]\n",
    "    return len_arr\n",
    "def vocab_size_corpus(tokenized_corpus):\n",
    "    merged_corpus = []\n",
    "    for l in tokenized_corpus:\n",
    "        merged_corpus += l\n",
    "    \n",
    "    return len(set(merged_corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word_length before stopword removal: 3.7567567567567566\n",
      "Length of texts before stopword removal: [9, 14, 14]\n",
      "Vocabulary size before stopword removal: 28\n",
      "Average word_length after stopword removal: 5.714285714285714\n",
      "Length of texts after stopword removal: [3, 6, 5]\n",
      "Vocabulary size after stopword removal: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Average word_length before stopword removal: {}\".format(avg_word_len_corpus(tokenized_example_corpus)))\n",
    "print(\"Length of texts before stopword removal: {}\".format(get_text_lengths(tokenized_example_corpus)))\n",
    "print(\"Vocabulary size before stopword removal: {}\".format(vocab_size_corpus(tokenized_example_corpus)))\n",
    "\n",
    "tokenized_example_corpus = [remove_stopwords(x) for x in tokenized_example_corpus]\n",
    "tokenized_example_corpus\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Average word_length after stopword removal: {}\".format(avg_word_len_corpus(tokenized_example_corpus)))\n",
    "print(\"Length of texts after stopword removal: {}\".format(get_text_lengths(tokenized_example_corpus)))\n",
    "print(\"Vocabulary size after stopword removal: {}\".format(vocab_size_corpus(tokenized_example_corpus)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word_length before lemmatization: 5.714285714285714\n",
      "Length of texts before lemmatization: [3, 6, 5]\n",
      "Vocabulary size before lemmatization: 13\n",
      "Average word_length after lemmatization: 5.571428571428571\n",
      "Length of texts after lemmatization: [3, 6, 5]\n",
      "Vocabulary size after lemmatization: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Average word_length before lemmatization: {}\".format(avg_word_len_corpus(tokenized_example_corpus)))\n",
    "print(\"Length of texts before lemmatization: {}\".format(get_text_lengths(tokenized_example_corpus)))\n",
    "print(\"Vocabulary size before lemmatization: {}\".format(vocab_size_corpus(tokenized_example_corpus)))\n",
    "\n",
    "tokenized_example_corpus = [lemmatize_text(x) for x in tokenized_example_corpus]\n",
    "tokenized_example_corpus\n",
    "\n",
    "print(\"Average word_length after lemmatization: {}\".format(avg_word_len_corpus(tokenized_example_corpus)))\n",
    "print(\"Length of texts after lemmatization: {}\".format(get_text_lengths(tokenized_example_corpus)))\n",
    "print(\"Vocabulary size after lemmatization: {}\".format(vocab_size_corpus(tokenized_example_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word_length before stemming: 5.571428571428571\n",
      "Length of texts before stemming: [3, 6, 5]\n",
      "Vocabulary size before stemming: 12\n",
      "Average word_length after stemming: 4.785714285714286\n",
      "Length of texts after stemming: [3, 6, 5]\n",
      "Vocabulary size after stemming: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Average word_length before stemming: {}\".format(avg_word_len_corpus(tokenized_example_corpus)))\n",
    "print(\"Length of texts before stemming: {}\".format(get_text_lengths(tokenized_example_corpus)))\n",
    "print(\"Vocabulary size before stemming: {}\".format(vocab_size_corpus(tokenized_example_corpus)))\n",
    "tokenized_example_corpus = [stem_text(x) for x in tokenized_example_corpus]\n",
    "tokenized_example_corpus\n",
    "\n",
    "print(\"Average word_length after stemming: {}\".format(avg_word_len_corpus(tokenized_example_corpus)))\n",
    "print(\"Length of texts after stemming: {}\".format(get_text_lengths(tokenized_example_corpus)))\n",
    "print(\"Vocabulary size after stemming: {}\".format(vocab_size_corpus(tokenized_example_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Feature Extraction\n",
    "Feature extraction is the process of transforming input data to a set of features to a format that can be used for machine learning. There are several different ways of doing this. Text classification is the focus of this workshop, therefore we have chosen to focus on bag-of-words feature extraction and 2 techniques for performing it. \n",
    "\n",
    "* Count Vectorization\n",
    "* Term Frequency Inverse Document Frequency representation.\n",
    "\n",
    "For feature extraction we will use the stemmed version of the corpus. As shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friend loyal book',\n",
       " 'wors crime burn book one read',\n",
       " 'discov librari went find librari']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_corpus = [' '.join(x) for x in tokenized_example_corpus]\n",
    "stemmed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Count Vectorization\n",
    "Count vectorization is technique where a set of texts is turned into a array of token counts. \n",
    "Information about the sklearn implementation can be found here: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector headers\n",
      "['book', 'burn', 'crime', 'discov', 'find', 'friend', 'librari', 'loyal', 'one', 'read', 'went', 'wors']\n",
      "count_array\n",
      "[[1 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 1 1 0 1]\n",
      " [0 0 0 1 1 0 2 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "count_vectorized_corpus = vectorizer.fit_transform(stemmed_corpus)\n",
    "print(\"Vector headers\")\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(\"count_array\")\n",
    "print(count_vectorized_corpus.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Term Frequency Inverse Document Frequency representation\n",
    "\n",
    "A tfidf-presentation prfers words that are frequent in the current document, but rare overall in the corpus. \n",
    "\n",
    "![alt text](https://mungingdata.files.wordpress.com/2017/11/equation.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://mungingdata.files.wordpress.com/2017/11/tfidf.png \"Logo Title Text w\")\n",
    "\n",
    "For more information about sklearns implementation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector headers\n",
      "['book', 'burn', 'crime', 'discov', 'find', 'friend', 'librari', 'loyal', 'one', 'read', 'went', 'wors']\n",
      "count_array\n",
      "[[0.4736296  0.         0.         0.         0.         0.62276601\n",
      "  0.         0.62276601 0.         0.         0.         0.        ]\n",
      " [0.32200242 0.42339448 0.42339448 0.         0.         0.\n",
      "  0.         0.         0.42339448 0.42339448 0.         0.42339448]\n",
      " [0.         0.         0.         0.37796447 0.37796447 0.\n",
      "  0.75592895 0.         0.         0.         0.37796447 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "tfidf_vectorized_corpus = tfidf_vectorizer.fit_transform(count_vectorized_corpus)\n",
    "print(\"Vector headers\")\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(\"count_array\")\n",
    "print(tfidf_vectorized_corpus.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Practice tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Task 1 - Text pre-processing\n",
    "The text below needs to be pre-processed to be ready for use. Identify and (if you have time) apply the techniques\n",
    "needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To Akousilaos@\\n , (?) finance-agent, greeting. Paeu3s2 son of Souneus12, assistant, has pre1paid\\n in accordance with a receipt (?) for the poll-tax of the 10th year of Tiberius Caesar Augustus at Tebtunis twelve drachmas of debased silver, total 12 dr. debased silver. The 10th year of Tiberius Caesar Augustus, Choiak 13. And on Pharmouthi 5 through Akousilaos at Tebtunis twelve drachmas for poll-tax, total 12 dr.'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"To Akousilaos@\\n , (?) finance-agent, greeting. Paeu3s2 son of Souneus12, assistant, has pre1paid\\n in accordance with a receipt (?) for the poll-tax of the 10th year of Tiberius Caesar Augustus at Tebtunis twelve drachmas of debased silver, total 12 dr. debased silver. The 10th year of Tiberius Caesar Augustus, Choiak 13. And on Pharmouthi 5 through Akousilaos at Tebtunis twelve drachmas for poll-tax, total 12 dr.\"\"\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Task 2 - Label normalization\n",
    "You are given a set of labels. Look at the labels and determine what needs to be done to standardize and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Between 300 and 130 B.C. \\n',\n",
       " 'Late 2nd or 3rd century A.D. \\n',\n",
       " 'Late 2nd century B.C. \\n',\n",
       " '181/182 or 213/214 A.D. \\n',\n",
       " 'Between 105 and 90 B.C. \\n',\n",
       " 'Between 300 and 130 B.C. \\n',\n",
       " '2nd century A.D. \\n',\n",
       " 'Between 300 and 130 B.C. \\n',\n",
       " '29 Oct. or 8 Nov. 114 B.C. \\n',\n",
       " '1st century A.D. \\n',\n",
       " '3rd century A.D. \\n',\n",
       " '2nd or 3rd century A.D. \\n',\n",
       " 'Early 2nd century B.C. \\n',\n",
       " 'Between 300 and 130 B.C. \\n',\n",
       " '2nd century B.C. \\n',\n",
       " '2nd century A.D. \\n',\n",
       " 'ca. 192/191 B.C.? \\n',\n",
       " 'Between 300 and 130 B.C. \\n',\n",
       " 'Early 2nd century B.C. \\n',\n",
       " 'Between 300 and 130 B.C. \\n']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = open(\"data/dates.txt\",\"r\").readlines()\n",
    "labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement your solution here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Task 3 - Find most significant words for each text in the given corpus\n",
    "Hint: Use tf-idf vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Never wear your best trousers when you go out to fight for freedom and truth.\",\n",
    "         \"The pillars of truth and the pillars of freedom - they are the pillars of society.\",\n",
    "          \"The strongest man in the world is he who stands most alone.\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement your solution here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
