{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A practical introduction to machine learning and natural language processing on papyrus data \n",
    "## By MSc. André Walsøe and dr. Andrea Gasparini University of Oslo\n",
    "\n",
    "In this workshop we will cover all the steps from downloading and preparing a dataset, filtering the data to training a classification model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1 Preparation and exploration of Dataset\n",
    "\n",
    "1. Install python libraries and download resources\n",
    "2. Download dataset from github repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download dataset and install libraries\n",
    "The dataset was created by downloading and filtering data from papyri.info.\n",
    "All the \"raw\" data can be found here:\n",
    "https://github.com/papyri/idp.data/tree/master/APIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Downloading dataset\n",
    "!curl -L -o data.tsv https://raw.githubusercontent.com/auwalsoe/encode_nlp_workshop_2023/main/data/papyrus_data.tsv\n",
    "\n",
    "## Installing libraries that are note provided in the default colab kernel.\n",
    "!pip install nltk\n",
    "!pip install gradio\n",
    "\n",
    "## Downloading ntlk resources for tokenization and stopword removal\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Data exploration and introduction to NLP techniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data exploration\n",
    "Data exploration is important in NLP and machine learning because it helps to understand the characteristics of the data that will be used to train and test the model. This includes understanding the distribution of the data, identifying patterns and trends, and identifying any potential issues such as missing values or outliers. Additionally, data exploration can help to identify any biases or errors in the data that may negatively impact the performance of the model. By performing data exploration, practitioners can gain insights that can be used to improve the quality of the data and the performance of the model.\n",
    "\n",
    "What we will cover here:\n",
    "1. Basic data exploration with pandas\n",
    "2. Application of filtering techniques based on data exploration findings\n",
    "3. Hands-on task: Data exploration and filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Basic Data exploration and filtering with pandas\n",
    "To load the dataset into our google colab we use a library called pandas (https://pandas.pydata.org) which is a open source data analysis and data manipulation tool that is widely used for every tasks involving data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the pandas Library\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the downloaded data into a pandas dataframe\n",
    "#data = pd.read_csv('/content/data.tsv', delimiter = '\\t')\n",
    "data = pd.read_csv('data.tsv', delimiter = '\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing column names\n",
    "for col in data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get frequencies of columns\n",
    "def plot_column_distribution(dataset, column_name, top_n):\n",
    "    \"\"\"Creates a bar plot of the frequency of different columns in the dataset.\n",
    "    Input variables:\n",
    "    dataset: Pandas dataframe\n",
    "    Column name: string\n",
    "    top_n: int\"\"\"\n",
    "    \n",
    "    dataset[column_name].value_counts()[:top_n].plot(kind=\"bar\")\n",
    "#data.category.value_counts()[:10].plot(kind='bar')\n",
    "\n",
    "plot_column_distribution(data, \"provenance\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Application of filtering techniques based on data exploration findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing data entries with unknown provenance from the dataset\n",
    "data = data[data.provenance != 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning all provenances with frequency less than 100 to \"other\"\n",
    "threshold = 100\n",
    "freq = data['provenance'].value_counts()\n",
    "mappings = freq.index.to_series().mask(freq < threshold, 'Other').to_dict()\n",
    "\n",
    "data['provenance'] = data['provenance'].map(mappings)\n",
    "\n",
    "\n",
    "translations = data['translation'].values\n",
    "provenance = data['provenance'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column_distribution(data, \"provenance\", 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Hands-on excercise: Data exploration and filtering\n",
    "Explore the different columns of the dataset, for example with the plot_column_distribution function, or data[COLUMN_NAME].value_counts() and propose other filters that should be applied to improve the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your solution or ideas below this line:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2.2: Introduction to nlp techniques\n",
    "1. Lower text \n",
    "2. Tokenization\n",
    "3. Stopword removal\n",
    "4. Vectorization\n",
    "6. Show how these techniques can be applied on papyrus data using NLTK\n",
    "7. Hands-on exercise: Apply NLP techniques to papyrus dataset\n",
    "\n",
    "### Example corpus\n",
    "A corpus is a structured set of texts which can be used for statistical analysis or similar. In this exercise we will use a small example corpus consisting of parts of 3 papyri translations that were chosen randomly. In this section I will show how we can apply the pre-processing methods shown above on a corpus. We will re-use the functions written above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_corpus = [\"Agathon to Patron, greeting. Apollonios has applied to me about the village of Takona, that you should appoint him as guard. you would therefore do well to hand (the post?) over to him. good-bye\", \n",
    "                  \"Address Asklepiades to Marres, greeting. Menches having been appointed village scribe of Kerkeosiris on the understanding that he shall cultivate at his own expense ten arouras of the land in the neighborhood of the village which has been reported as unproductive at a rent of fifty artabas\", \n",
    "                  \"I therefore present to you this complaint in order that the accused may be summoned and compelled to refund me the damage; and if he refuses i beg you to forward a copy of the petition to the proper officials, so that I may have it placed on record and the king may incur no loss. Farewell.\"]\n",
    "example_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Lower text\n",
    "Here it is shown how to lower the text. In a small dataset it is an important step, as the word will be interpreted as 2 different words by the algorithm if the word is written with capital letter or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_corpus = [x.lower() for x in example_corpus]\n",
    "example_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Tokenization\n",
    "Tokenization is a way of splitting a text into smaller units, called tokens, in this example we will split it into words, but one can also split into sentences, letters or other types of units. The reason for doing this is that most NLP models and algorithms work on a token level. The tokens are also used to create the vocabulary, the set of unique tokens in the corpus.\n",
    "\n",
    "In our example we use the word_tokenize method from the Natural language toolkit library (NLTK): https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_example_corpus = [word_tokenize(x) for x in example_corpus]\n",
    "tokenized_example_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Stopword removal\n",
    "After tokenizing the corpus the next step is to remove the stopwords. Stopwords are words that are words that are frequently used in a language. In english it can be words like:\n",
    "- i\n",
    "- me\n",
    "- my\n",
    "- myself\n",
    "- we\n",
    "- our\n",
    "- ours\n",
    "- ourselves\n",
    "- you\n",
    "- your\n",
    "- yours\n",
    "- yourself\n",
    "- yourselves\n",
    "- he\n",
    "\n",
    "\n",
    "Full list can be seen here: https://gist.github.com/sebleier/554280\n",
    "\n",
    "Removing stopwords can improve the performance of text classification models in several ways:\n",
    "\n",
    "- Dimensionality reduction: Stopwords can make up a large proportion of the words in a text, removing them can reduce the dimensionality of the data and make the model more efficient.\n",
    "\n",
    "- Improved feature selection: Stopwords do not carry much meaning, so removing them can help the model to focus on the most meaningful words or phrases in the text, which can improve feature selection.\n",
    "\n",
    "- Better generalization: Removing stopwords can help the model to generalize better, as it will not be distracted by common words that may not be indicative of the class of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def remove_stopwords(tokenized_text, stopwords):\n",
    "    clean_tokenized_text = [x for x in tokenized_text if x not in stopwords.words('english')]\n",
    "    return clean_tokenized_text\n",
    "\n",
    "tokenized_example_corpus_without_stopwords = [remove_stopwords(x, stopwords) for x in tokenized_example_corpus]\n",
    "tokenized_example_corpus_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Vectorization\n",
    "Vectorization describes the way to convert data from raw text into vectors of numbers that can be input to machine learning models. This is a key part of feature extraction. There are several ways of converting the data, but in this workshop we will cover only 2:\n",
    "- Count\n",
    "- Tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorization\n",
    "Count vectorization is a technique used in NLP to convert a collection of text documents into a numerical representation, such as a matrix or array. The technique involves tokenizing the text (i.e., breaking it up into individual words or phrases) and counting the occurrences of each token in each document. The resulting matrix or array can then be used as input to a machine learning algorithm for tasks such as text classification or clustering. The columns of the matrix represent the features (i.e., tokens) and the rows represent the samples (i.e., documents). Count vectorization is a simple and effective method for representing text data, but it can be high-dimensional and sparse.\n",
    "\n",
    "For more information about how it is implemented in Sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CountVect = CountVectorizer(stop_words='english').fit(example_corpus)\n",
    "\n",
    "example_corpus_count_vectorized = CountVect.transform(example_corpus)\n",
    "print(CountVect.get_feature_names_out())\n",
    "print(example_corpus_count_vectorized.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf vectorization\n",
    "Tf-idf stands for term frequency-inverse document frequency. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "How it is calculated:\n",
    "\n",
    "![alt text](https://mungingdata.files.wordpress.com/2017/11/equation.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://mungingdata.files.wordpress.com/2017/11/tfidf.png \"tfidf formula\")\n",
    "\n",
    "For more information about sklearns implementation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "## Initiating the TFIDF vectorizer and adapting it to our example corpus. Here we also remove the stopwords.\n",
    "TfidfVect = TfidfVectorizer(stop_words='english').fit(example_corpus)\n",
    "\n",
    "## Transforming the dataset into a tfidf vectorized representation\n",
    "example_corpus_tfidf_vectorized = TfidfVect.transform(example_corpus)\n",
    "print(TfidfVect.get_feature_names_out())\n",
    "print(example_corpus_tfidf_vectorized.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Hands-on exercise: Apply NLP techniques to papyrus dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 3: Building a text classification model\n",
    "1. Building a text classification model\n",
    "    1. Choose what to predict and which variables to use\n",
    "    2. Split data into training and test\n",
    "    3. Transform/vectorize data\n",
    "    4. Train a logistic regression model\n",
    "    5. Test and evaluate metrics\n",
    "    6. Deploy model with Gradio\n",
    "4. Hands-on exercise: Build and train your own classification model (deploy it with Gradio if time allows)\n",
    "5. Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Building a text classification model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Choose what to classify and data to train the model with\n",
    "As we saw in 2.1.2 we have several different types of data columns:\n",
    "- translation\n",
    "- category\n",
    "- author\n",
    "- summary\n",
    "- keywords\n",
    "- originDate\n",
    "- provenance\n",
    "\n",
    "Which means that there are many possible data inputs we can use in order to build a model.\n",
    "Classification is about predicting a class, by identifying which classes belongs to it based on different parameters.\n",
    "In a classification model we need input variables x and a label y that we want to predict.\n",
    "\n",
    "In this example (since we are doing NLP), I will use translations as the input data. With the translations we could classify author, category, provenance or originData. Here I will show how to build a model that tries to classify a papyrus provenance based on it's translation. \n",
    "\n",
    "In the hands-on session you can try to change what to classify and see how it works :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Split data into training and test\n",
    "Splitting the data into a training set and a test set is an important step in the machine learning process, as it helps to ensure that the model is able to generalize well to new, unseen data.\n",
    "\n",
    "The training set is used to train the model, while the test set is held back as a way to evaluate the model's performance. By using a separate test set, we can get an estimate of how well the model is likely to perform on new, unseen data, rather than just how well it performs on the data it was trained on.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1400/1*Nv2NNALuokZEcV6hYEHdGA.png \"Logo Title Text 1\")\n",
    "\n",
    "Normally the split is 60% training set, 20% validation, 20% test, but this depends on the size of the dataset. In this workshop we are splitting it into 80% training set and 20% test set. Validation sets are often used in order to tune the hyperparameters of the model, and in this course it will not be covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(translations, provenance, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Transform/vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "## Choose vectorization method:\n",
    "vect_type = \"tfidf\"\n",
    "if vect_type == \"tfidf\":\n",
    "    ## Initializing a TFidf vectorizer. Here we also specify that the vectorizer should lowercase the text and remove stopwords.\n",
    "    tfidf_vect = TfidfVectorizer(lowercase= True, analyzer = \"word\", stop_words=\"english\")\n",
    "\n",
    "    ## Adapting the vectorizer to our training set and then transforming our dataset to tfidf_vectorization. I.e the vectorizer vocabulary will be based on the training set.\n",
    "    X_train_vectorized= tfidf_vect.fit_transform(X_train)\n",
    "\n",
    "    ## Transforming our dataset to tf_idf\n",
    "    X_test_vectorized = tfidf_vect.transform(X_test)\n",
    "\n",
    "    vectorizer = tfidf_vect\n",
    "else:\n",
    "    ## Initializing a CountVectorizer. Here we also specify that the vectorizer should lowercase the text and remove stopwords.\n",
    "    count_vect = CountVectorizer(lowercase=True, analyzer=\"word\", stop_words=\"english\")\n",
    "\n",
    "    ## Adapting the vectorizer to our training set and then transforming our dataset to tfidf_vectorization. I.e the vectorizer vocabulary will be based on the training set.\n",
    "    X_train_vectorized= count_vect.fit_transform(X_train)\n",
    "\n",
    "    ## Transforming our dataset to tf_idf\n",
    "    X_test_vectorized = count_vect.transform(X_test)\n",
    "    \n",
    "    vectorizer = count_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Train a logistic regression model\n",
    "In this workshop we train and predict using a logistic regression model. This type of model is widely used, and is very efficient and not too computationally demanding. It performs well on classification tasks, but it is not State-of-the-art, meaning that there are more complex and modern algorithms that perform better. Other alternatives that perform better are for example deep learning algorithms based on transformer architectures.\n",
    "\n",
    "For a more in-depth (but practical) introduction to logistic regression one can read [here](https://towardsdatascience.com/the-perfect-recipe-for-classification-using-logistic-regression-f8648e267592)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train_vectorized,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5 Test and evaluate metrics\n",
    "To test how well our model is performing we run the classifier on the data in our test set and then we compare classifications made by our model with the true labels that the test set data has. Based on this we can calculate several metrics to evaluate how well the model is performing.\n",
    "In this test we will use F1, Accuracy, Precision and Recall.\n",
    "\n",
    "To learn more about precision and recall: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/f0453f2614cd29f5dd49c2c9a0ef807985128e9e \"Logo Title Text 1\") \n",
    "\n",
    "![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/f5c869c51dba6f1df65a6e6630c516de161632d4 \"Logo Title Text 1\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicting provenances for our test input.\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Comparing our predicted output with the true labeled provenances and calculating metrics for each provenance.\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.6 Deploy model with Gradio                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "gradio_model = model\n",
    "gradio_vectorizer = vectorizer\n",
    "def deploy_my_model(text):\n",
    "    text_vectorized = gradio_vectorizer.transform([text])\n",
    "    return str(gradio_model.predict(text_vectorized)[0])\n",
    "demo = gr.Interface(fn = deploy_my_model, inputs= gr.Textbox(lines=3,placeholder=\"Papyrus translation here\"), outputs=\"text\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Hands-on exercise: Reflect on possible applications for these tools in your field.\n",
    "Today you have received a short introduction to a lot of subjects in machine learning and NLP. As the last part of this workshop I want us all to brainstorm on possible applications of these tools and techniques in your field.\n",
    "\n",
    "Please write up your ideas and we will share them in the class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for participating in the workshop!\n",
    "Any feedback or question is very welcome at auwalsoe@gmail.com\n",
    "\n",
    "and if you enjoyed the workshop and want to save it for later, feel free to give it a star on github!\n",
    "\n",
    "All the best,\n",
    "André Walsøe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81b04b4b00d0fce4f4c274cf252073ab071336a98a82b44be1ec0d88a271d631"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
